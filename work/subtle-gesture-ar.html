<!DOCTYPE html>
<!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Wed Oct 30 2019 01:26:31 GMT+0000 (UTC)  -->
<html data-wf-page="5db722ecb2122a3c55a1a0a3" data-wf-site="5d8c11000558613f561603d7">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-146491716-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-146491716-2');
  </script>

  <meta charset="utf-8">
  <title>subtle gesture AR</title>
  <meta content="subtle gesture AR" property="og:title">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="../css/normalize.css" rel="stylesheet" type="text/css">
  <link href="../css/webflow.css" rel="stylesheet" type="text/css">
  <link href="../css/whitney-li-portfolio.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="../images/favicon.png" rel="shortcut icon" type="image/x-icon">
  <link href="../images/webclip.png" rel="apple-touch-icon">
</head>
<body>
  <div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-nav">
    <div class="navigation-items"><a href="../index.html" class="logo-link w-nav-brand"><img src="../images/whitney-logo.png" width="58" alt="" class="logo-image"></a>
      <div class="navigation-wrap">
        <nav role="navigation" class="navigation-items w-nav-menu"><a href="../index.html" class="navigation-item w-nav-link">Portfolio</a><a href="../about.html" class="navigation-item w-nav-link">About</a></nav>
        <div class="menu-button w-nav-button"><img src="../images/menu-icon_1menu-icon.png" width="22" alt="" class="menu-icon"></div>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="w-layout-grid project-overview-grid">
      <div id="w-node-f2c6de040bbf-55a1a0a3">
        <h1 class="heading-jumbo">Subtle Gesture for Everyday AR</h1>
        <div class="paragraph-light">Web Design</div>
      </div>
      <div id="w-node-f2c6de040bc4-55a1a0a3">
        <div class="position-name-text">Project Overview </div>
        <div class="paragraph-light cc-position-name">As wearable become more and more common in our lives, we want them to augment our experience in conversations or social interactions rather than hinder them. There is a lot of potential to multitask while wearing head worn displays like Google Glass, but using such devices can be distracting to any peers or bystanders. Operating head worn devices using subtle gestures can aid in multitasking and task completion without disrupting the surrounding environment. The overarching goal of this project is to create a UI interface for  Google Glass and use subtle gestures to operate it in an everyday scenario. These gestures should be subtle or socially acceptable.
       </div>
      </div>
      <div id="w-node-f2c6de040bc9-55a1a0a3">
        <div class="position-name-text">Current Work</div>
        <div class="paragraph-light cc-position-name">For this project, we conducted multiple user interviews to understand the needs and pain points of different user groups in various restaurant scenarios, and decided to focus on cashiers at fast food restaurants for our first prototype. We then developed a storyboard to visualize the ideal use case, discussed what gestures would be most natural and intuitive for different interactions, designed our prototype of the AR interfaces on Google Glass, and recorded the video at McDonald’s.  </div>
      </div>
    </div>

    <div class="section">
      <h1 class="heading-7">Stage One</h1>
      <h3>User Interviews</h3>
      <p>In order to get more of an insight into the pain points of the dining experience we interviewed 5 different types of dining scenarios: Fast Food, Cafe, Low-End Buffet, High-End Buffet and Fine Dining. Our interview goals were to understand different sales people in different dining scenarios and understand their  needs, pain points and interactions in the workplace to deduce suitable uses for subtle AR gestures.<br /><br />
        During the user interview stage, we discovered two major themes. Firstly, cashiers or waiters at different restaurants we interviewed all have the need for immediate information which can be provided by Google Glass, but the kind of information they needed are different based on context. For example, waiters at campus cafe sometimes forget the seating and orders of the customers when it gets too crowded, while waiters at high-end seafood restaurant need to stay updated with the long and frequently changing menu and the origins of different fish. For the purpose of fast prototyping and testing, we chose cashiers at fast food restaurants as our scenario, but other scenarios may have more complicated and urgent information needs, higher expectations of customers, and thus more in need of subtle gestures with AR applications. <br /><br />
        Secondly, expectations of customers and waiters can be different. For example, while waiters at high-end restaurants can benefit from having the menu and customer information provided by Google Glass, customers who go to high-end restaurants may want to receive personable and professional services and can be disturbed by seeing waiters wearing Google Glass and making confusing gestures. This point is also why we decided to shoot the videos from both the customer and the cashier’s perspectives. <br /><br />
      </p>

      <h3>Prototype Design</h3>
      <p>To decide what features are needed for the cashier to complete a smooth transaction without looking down or diverting his attention from the customers, we wrote down the key steps that a cashier  needs to go through to place an order: greet customers; recommend food if they need; let them order food; check for storage; follow up with customers; complete payment. <br /><br />
        Based on the identified steps, we discussed what information should be exchanged between the customer and the cashier, how it should look like on Google Glass interfaces, and what gestures are appropriate for different actions. Specifically, we decided to use self-sync gesture (head nodding and wrist twisting) for initiation, double tap on the ring for confirmation, swipe the ring for switching options, and long tap on the ring for cancellation. <br /><br />
      </p>

      <h3>Video Production/ Prototype Testing</h3>
      <p>The video production stage is also considered to be the pilot user testing of this prototype, where the actors who are intentionally not involved in the prototype design stage experienced the prototype from the cashier and customer’s perspectives. The student portraying cashier commented that he found the gestures used for the scenario are relatively intuitive and easy to learn. Furthermore, because cashiers usually stand behind the counter, it is easy to do gestures to control Google Glass without getting any notice. <br /><br />
        However, the student portraying customer found her ordering experience not so natural. When she went to fast food restaurants, she expected the cashier to pause and look down to place her order, but this time the cashier was making eye contact with her all the time, which made her feel awkward and wondered if her order was placed successfully. <br /><br />
        Furthermore, when comparing the video with previous observations of cashiers at fast food restaurants, we noticed that using subtle gestures with Google Glass can be hard for cashiers to chat with customers, because the interactions would be led by the AR interfaces and chats may disturb the speech recognition feature of Google Glass.
      </p>
      <div class="image-block">
        <div class="w-video w-embed"></div>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/-JZIUlPQzms" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</iframe>
        <div class="text-under-image">Video showing our prototype in a real-life scenario</div>
      </div>
    </div>

    <div class="section">
      <h1 class="heading-7">Stage Two</h1>
      <p>
        Work in Progress
      </p>
    </div>


  <div class="footer-wrap">
    <div class="div-block-5"><img src="../images/up.png" width="31" alt="" class="image-5"><a href="../index.html" class="logo-link w-nav-brand"><img src="../images/whitney-logo.png" width="58" alt="" class="logo-image image-4"></a></div>
    <div class="footer-links"><a href="https://www.linkedin.com/in/yuntongli/" target="_blank" class="footer-item">Linkedin</a><a href="https://www.instagram.com/whitneyli1218/" target="_blank" class="footer-item">Instagram</a><a href="https://medium.com/@whitneyli" target="_blank" class="footer-item">Medium</a></div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <script src="../js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>
